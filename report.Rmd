---
title: "Final Report"
author: 
  "Team 26: Dipak Krishnan, Santiago Roa"
date: "May 3rd, 2019"
header-includes:
   \usepackage{setspace}
   \doublespacing
output:
  pdf_document:
    toc: yes
    number_sections: TRUE
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE) # show code
knitr::opts_chunk$set(warning = FALSE) # hide warnings
knitr::opts_chunk$set(message = FALSE) # hide messages
knitr::opts_chunk$set(cache = TRUE) # cache results 
```

```{r,echo=FALSE}
price = read.csv("price.csv")
review = read.csv("review.csv")
price.t = read.csv("price_test.csv")
review.t = read.csv("review_test.csv")

### process data

price.pre = price
price.pre$host_is_superhost = as.factor(as.numeric(price.pre$host_is_superhost) -1)
price.pre$host_identity_verified = as.factor(as.numeric(price.pre$host_identity_verified) -1)
price.pre$instant_bookable = as.factor(as.numeric(price.pre$instant_bookable) -1)
price.pre$host_response_rate = as.numeric(sub("%", "", price.pre$host_response_rate)) / 100
price.pre$host_response_time = as.factor(unclass(price.pre$host_response_time) -1)
price.pre$cleaning_fee = as.numeric(sub("$", "", price.pre$cleaning_fee, fixed = T))
price.pre$price = as.numeric(sub("$", "", price.pre$price, fixed = T))
price.pre$cleaning_fee[is.na(price.pre$cleaning_fee)] = 0
price.pre$price[is.na(price.pre$price)] = 0
price.pre$cancellation_policy = as.factor(as.numeric(price.pre$cancellation_policy))

review.pre = review
review.pre$host_is_superhost = as.factor(as.numeric(review.pre$host_is_superhost) -1)
review.pre$host_identity_verified = as.factor(as.numeric(review.pre$host_identity_verified) -1)
review.pre$instant_bookable = as.factor(as.numeric(review.pre$instant_bookable) -1)
review.pre$host_response_rate = as.numeric(sub("%", "", review.pre$host_response_rate)) / 100
review.pre$host_response_time = as.factor(unclass(review.pre$host_response_time) -1)
review.pre$cleaning_fee = as.numeric(sub("$", "", review.pre$cleaning_fee, fixed = T))
review.pre$price = as.numeric(sub("$", "", review.pre$price, fixed = T))
review.pre$cleaning_fee[is.na(review.pre$cleaning_fee)] = 0
review.pre$price[is.na(review.pre$price)] = 0
review.pre$cancellation_policy = as.factor(as.numeric(review.pre$cancellation_policy))

library(stringr)

amenities = sapply(price.pre$amenities, FUN=function(x) {
  # some function that just splits up all the words in each row of amenities
  s = toString(x)
  s = strsplit(s[[1]],",")
  newS = str_replace(s[[1]],'\\{',"")
  newS = str_replace(newS,'\\}',"")
  newS = str_replace(newS,'\"',"")
  newS = str_replace(newS,'\"',"")
  tolower(newS)
})

amenities.review = sapply(review.pre$amenities, FUN=function(x) {
  # some function that just splits up all the words in each row of amenities
  s = toString(x)
  s = strsplit(s[[1]],",")
  newS = str_replace(s[[1]],'\\{',"")
  newS = str_replace(newS,'\\}',"")
  newS = str_replace(newS,'\"',"")
  newS = str_replace(newS,'\"',"")
  tolower(newS)
})

num.amenities = c()
for(i in 1:length(price.pre$amenities)) {
  num.amenities[i] = length(amenities[[i]])
}

num.amenities.rev = c()
for(i in 1:length(review.pre$amenities)) {
  num.amenities.rev[i] = length(amenities.review[[i]])
}

price.pre$num_amenities = num.amenities
review.pre$num_amenities = num.amenities.rev

###

n <- nrow(price.pre)
sample.price <- price.pre[sample(n), ]
train.indices <- 1:round(0.7 * n)
train <- sample.price[train.indices, ]
test.indices <- (round(0.7 * n) + 1):n
test <- sample.price[test.indices, ]

nR <- nrow(review.pre)
sample.review <- review.pre[sample(nR), ]
train.indices.rev <- 1:round(0.7 * nR)
trainR <- sample.review[train.indices.rev, ]
test.indices.rev <- (round(0.7 * nR) + 1):nR
testR <- sample.review[test.indices.rev, ]

price.pre = price.pre[,-c(11,19)]
review.pre = review.pre[,-c(10,19)]
```

# Introduction

The main dataset we are working with in this project has data on Airbnb listings in
Seattle. Through analyzing this data, we are trying to fulfill two main objectives. First, we are
trying to determine whether several host and housing quality measurements have an effect on
the price of a given listing. Second, we are attempting to determine whether these same
measurements have an effect on the review score of the given listing. The first task is a
regression based task and the second is a classification based solution. Our tasks have two
different datasets, of which the price dataset has 5200 observations and 25 variables, including
our response variable $\it{price}$. Our review dataset has 4043 observations and 26 variables,
including our response variable $\it{review\_scores\_rating}$. Initially, there were 48 NA values in the
price dataset and 34 NA values in the review dataset. These NA variables were entirely
contained in 2 columns, $\it{price}$ and $\it{cleaning\_fee}$. We treated the NA values by converting them
to 0, meaning that there was no price or cleaning fee for a given listing.


# Exploration

We begin by taking a look at the different neighborhoods in Seattle. The table below summarizes the average price per neighborhood and shows the relative proportion of the price of each neighborhood in both datasets. About 18% of the listings alone are located in Downtown Seattle. On average, the price per listing is about 60 dollars more expensive there than the overall average. Similarly, in Queen Anne, a notoriously expensive area in Seattle, the average price is about 30 dollars more expensive than the overall average. This difference in price hints at the importance of location for an Airbnb listing. 

```{r}
# for (i in 1:17) {
#   p = which(price.pre$neighbourhood_group_cleansed == )
#   r = which(review.pre$neighbourhood_group_cleansed == l)
#   price.mean = mean(price.pre$price)
#   review.mean = mean(review.pre$price)
# }

library(plyr)
neighborhood.means = aggregate(price.pre[,"price"], list(price.pre$neighbourhood_group_cleansed), mean)
n.means = aggregate(review.pre[,"price"], list(review.pre$neighbourhood_group_cleansed), mean)

n = names(table(price.pre$neighbourhood_group_cleansed))
prop = ddply(price.pre, .(neighbourhood_group_cleansed), summarize, prop = 100 * length(neighbourhood_group_cleansed) / nrow(price.pre))

a = cbind("mean price for price" = neighborhood.means[,2], "mean price for review" = n.means[,2], "proportion" = prop[,2])
rownames(a) = n
a
```


Each listing in the dataset included its unique latitude and longitude coordinates. To make the latitude and longitude measurements more useful and to improve the feature set, we tabulated the latitude/longitude coordinates of five Seattle places of interest: downtown, the Space Needle, the Gum Wall, Pike Place Market, and the Great Wheel. We wrote a script that calculated, for each listing, the distance between the listing and each of the five important locations in Seattle. This provided context to our task because it established how well-located certain listings were in comparison to others.

More specifically, for both the regression and classification task, predicting price and classifying
review_scores_rating in the best way required first looking at the data and evaluating the
feature set. Both datasets contained the same feature set with the review dataset containing an
extra variable - our response review_scores_rating for the classification task.

We continued by looking for potentially problematic features in the dataset. In the price
dataset, there were two: amenities and room_type. The amenities variable, for each listing, lists
the specific amenities that the listing has such as wifi access, TV availability and air
conditioning. The room_type variable lists the type of the room offered by a given listing. These
two features presented due to the number of levels of the factor variables.

Each index in the amenities column contained a dictionary containing that specific
listing’s amenities. Logically, we decided to test if the number of amenities for a given listing
would affect its price instead of the amenities themselves. We parsed the amenities column
and produced a new feature num_amenities that reported the total number of amenities for
each listing. We dropped the original amenities column from our analysis, and dropped the
room type column as well from our analysis, as the ordering of its levels presented problems in
the model creation process. We kept all other variables that were initially in the dataset in some
form in our analysis.

Finally, multiple features in the dataset required recoding. For instance, we refactored
the host_is_superhost, host_identity_verified, and instant_bookable variables that were
originally true/false variables coded as “t” and “f” into 0/1 variables with 1 coded as true. ##
maybe line of code here and below ## The host_response_rate variable was a percentage in
the original dataset, and was changed into a proportion. Two variables: host_response_time
and cancellation_policy were refactored into categorical variables with numbers representing
levels of factors instead of words. Our response variable price and the cleaning_fee variable
contained the “$” character before their values - this character was removed and the predictors
were converted to numeric variables.

## Regression Model Selection

At this point, after processing and cleaning the data, we had a feature set that we felt
good about. To move forward with the regression task, we knew that some variables had linear
correlation with our response variable price, namely cleaning_fee and beds. It seemed that
fitting a multiple linear regression model was a logical place to start as a baseline technique for
prediction.

To begin, we split our price dataset into a training and test set with 75% of data in the
price dataset allocated to training and 25% allocated to a validation test set. We fit a multiple
linear regression model, ## code here ## regressing price against all the variables in our final,
processed training set. The goodness of the fit was evaluated in two ways: against our
regression assumptions and with cross-validated mean squared error. The residuals vs fitted
values plot of the fitted regression object showed a cluster of points around 0 with high
outliers, potential influential points, and an uneven distribution. Our baseline linear model was
not optimal for the dataset, and we inferred another technique could produce a better fit. The
cross-validated error for the linear model using k-fold cross validation was around 11,000. ##
check this number ##

With our baseline in hand, we logically assumed that it’s possible the fit could be
improved by improving the bias-variance tradeoff we were dealing with. To make the model
more generalizable and get a sense for which predictors were important, we fit a ridge and
lasso model on our training set. We used ## code here cv.glmnet ## cross validation
techniques to establish the optimal lambda penalty value for both the ridge and lasso
regression techniques. Our cross-validated mean squared errors for both ridge and lasso were
around 8,000 which was an improvement on our baseline model. ## check this number ##
We further explored trees and random forests. Given the data had a mix of categorical
and numeric predictors, with some outliers in the data, trees and random forests provide a
robust measure that does not lose predictive power to high outliers and non-linearity. We fit a
normal decision tree, and a pruned version of the tree. ## code here ## From the plot of the
decision tree we chose a cp parameter that minimized relative error. The cross validated mean
squared errors both the normal decision tree and pruned tree improved upon ridge/lasso and
were around 6,000.

Random forests are an algorithmic improvement on decision trees, due to their greedy
search on a subset of features for the best split to make rather than evaluating all features in a
decision tree. In addition, random forests retain the strengths of decision trees in working with
numeric and categorical predictors and generally produce low bias, moderate variance models.

We fit a random forest with 500 trees as a baseline on our training set. ## code here ##
Random forests had the lowest cross-validated mean squared error around 4000. The
predictions that the random forest produced were the closest to the price of the listings in our
training set. TuneRF here?

## Classification Model Selection


# Analysis of Results

